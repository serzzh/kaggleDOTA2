{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphlab as gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = pd.read_csv(\"features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_check = pd.read_csv(\"features_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#features_check.print_rows(num_rows=3, num_columns=109)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Removing irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_vector = list(features_check.columns)[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Reshaping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = features.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = features\n",
    "y = data['radiant_win']\n",
    "del(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_heroes = len(data['r1_hero'].unique()) \n",
    "r_new = []\n",
    "for i in xrange(num_heroes):\n",
    "    r_new += ['r'+str(i+1)+'hero','r'+str(i+1)+'level', 'r'+str(i+1)+'xp', 'r'+str(i+1)+'gold', 'r'+str(i+1)+'lh',\n",
    "             'r'+str(i+1)+'kills','r'+str(i+1)+'deaths','r'+str(i+1)+'items', 'd'+str(i+1)+'hero', 'd'+str(i+1)+'level', \n",
    "             'd'+str(i+1)+'xp', 'd'+str(i+1)+'gold', 'd'+str(i+1)+'lh','d'+str(i+1)+'kills','d'+str(i+1)+'deaths',\n",
    "             'd'+str(i+1)+'items']\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=data.fillna(0)\n",
    "r1 = data['r1_hero']*16-16\n",
    "r2 = data['r2_hero']*16-16\n",
    "r3 = data['r3_hero']*16-16\n",
    "r4 = data['r4_hero']*16-16\n",
    "r5 = data['r5_hero']*16-16\n",
    "d1 = data['d1_hero']*16-16\n",
    "d2 = data['d2_hero']*16-16\n",
    "d3 = data['d3_hero']*16-16\n",
    "d4 = data['d4_hero']*16-16\n",
    "d5 = data['d5_hero']*16-16\n",
    "r = np.transpose([r1,r2,r3,r4,r5,d1,d2,d3,d4,d5])\n",
    "names = list(data.columns.values)\n",
    "row = []\n",
    "col = []\n",
    "val = []\n",
    "for i in xrange(len(data)):\n",
    "    for k in xrange(5):\n",
    "        for j in xrange(8):\n",
    "            row.insert(j+8*k+i*1000,i)\n",
    "            col.insert(j+8*k+i*1000,r[i,k]+j)\n",
    "            val.insert(j+8*k+i*1000,data[names[3+j+8*k]][i])\n",
    "            row.insert(j+8*(k+5)+i*1000,i)\n",
    "            col.insert(j+8*(k+5)+i*1000,r[i,(k+5)]+j)\n",
    "            val.insert(j+8*(k+5)+i*1000,-data[names[3+j+8*(k+5)]][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nele=len(data)\n",
    "nbus=np.amax(r)+8\n",
    "sp_features = sp.sparse.coo_matrix((val, (row,col)), shape=(nele, nbus), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_sparse_csr(filename,array):\n",
    "    np.savez(filename,data = array.data ,indices=array.indices,\n",
    "             indptr =array.indptr, shape=array.shape )\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return sp.sparse.csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "                         shape = loader['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sp_csr = sp_features.tocsr()\n",
    "save_sparse_csr('X_sparse',sp_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sp_csr = load_sparse_csr('X_sparse.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:3: FutureWarning: convert_objects is deprecated.  Use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "data_sub=data[name_vector]\n",
    "data_sub=data_sub[name_vector[81:]]\n",
    "data_sub['first_blood_player2']=data_sub['first_blood_player2'].convert_objects(convert_numeric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sub=data_sub.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97230, 20)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row1 = []\n",
    "col1 = []\n",
    "val1 = []\n",
    "names1 = list(data_sub.columns.values)\n",
    "for i in xrange(len(data_sub.columns)):\n",
    "    val1.extend(data_sub[names1[i]])\n",
    "    row1.extend(range(len(data_sub)))\n",
    "    col1.extend(i*np.ones(len(data_sub)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nele1=len(data)\n",
    "nbus1=len(data_sub.columns)\n",
    "X_add = sp.sparse.coo_matrix((val1, (row1,col1)), shape=(nele1, nbus1), dtype=float)\n",
    "X_csr = X_add.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = sp.sparse.hstack([X_csr,sp_csr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<97230x20 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 1944600 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<97230x1784 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 7778400 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<97230x1804 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 9723000 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(X_csr)\n",
    "del(sp_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#D=pd.DataFrame(X.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#D.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Splitting train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "...     X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Logistics regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1 = LogisticRegression(C=0.001, penalty='l2', tol=0.0001, random_state=142)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=142, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.754245909923\n"
     ]
    }
   ],
   "source": [
    "pred1 = model1.predict_proba(X_test.toarray())[:, 1]\n",
    "print roc_auc_score(y_test, pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.764703905781\n"
     ]
    }
   ],
   "source": [
    "pred1 = model1.predict_proba(X_train.toarray())[:, 1]\n",
    "print roc_auc_score(y_train, pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model1.pkl',\n",
       " 'model1.pkl_01.npy',\n",
       " 'model1.pkl_02.npy',\n",
       " 'model1.pkl_03.npy',\n",
       " 'model1.pkl_04.npy']"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from sklearn.externals import joblib\n",
    ">>> joblib.dump(model1, 'model1.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ">>> from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "X_sample, X_t, y_sample, y_t = train_test_split(\n",
    "...     X_train, y_train, test_size=0.9, random_state=42)\n",
    "\n",
    "param_grid = {'C': [0.0001, 0.001,  0.01, 0.1, 1] }\n",
    "clf1 = GridSearchCV(cv=4,estimator=LogisticRegression(C=0.001, intercept_scaling=1, \n",
    "dual=False, fit_intercept=True, penalty='l1', tol=0.0001), param_grid=param_grid)\n",
    "clf1.fit(X_train, y_train)\n",
    "clf1.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rates = [0.3,]\n",
    "n=300\n",
    "loss_matrix=range(n)\n",
    "for rate in rates:\n",
    "    clf = GradientBoostingClassifier(n_estimators=n, verbose=True, random_state=241, learning_rate=rate)\n",
    "    clf.fit(X_train,y_train)\n",
    "    loss_test=[]\n",
    "    loss_train=[]\n",
    "    min_loss=1\n",
    "    i_min=0\n",
    "    for i, y_decision in enumerate(clf.staged_decision_function(X_test.toarray())):\n",
    "        y_pred = 1.0 / (1.0 + np.exp(-y_decision))\n",
    "        l = roc_auc_score(y_test, y_pred)\n",
    "        loss_test.append(l)\n",
    "        if l<min_loss and rate==0.2:\n",
    "            min_loss=l\n",
    "            i_min=i\n",
    "    for i, y_decision in enumerate(clf.staged_decision_function(X_train.toarray())):\n",
    "        y_predt = 1.0 / (1.0 + np.exp(-y_decision))\n",
    "        lt = roc_auc_score(y_train, y_predt)\n",
    "        loss_train.append(lt)\n",
    "\n",
    "    loss_matrix=np.vstack([loss_matrix,loss_test,loss_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3686            5.17m\n",
      "         2           1.3591            4.91m\n",
      "         3           1.3518            4.77m\n",
      "         4           1.3460            4.70m\n",
      "         5           1.3403            4.61m\n",
      "         6           1.3362            4.54m\n",
      "         7           1.3324            4.49m\n",
      "         8           1.3289            4.44m\n",
      "         9           1.3253            4.44m\n",
      "        10           1.3223            4.40m\n",
      "        20           1.2972            4.18m\n",
      "        30           1.2787            3.99m\n",
      "        40           1.2635            3.83m\n",
      "        50           1.2509            3.66m\n",
      "        60           1.2395            3.51m\n",
      "        70           1.2286            3.36m\n",
      "        80           1.2189            3.21m\n",
      "        90           1.2096            3.06m\n",
      "       100           1.2017            2.91m\n",
      "       200           1.1366            1.44m\n",
      "       300           1.0889            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(init=None, learning_rate=0.3, loss='deviance',\n",
       "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "              presort='auto', random_state=241, subsample=1.0,\n",
       "              verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = GradientBoostingClassifier(n_estimators=300, verbose=True, random_state=241, learning_rate=0.3)\n",
    "model2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.754245909923\n",
      "0.717627460693\n"
     ]
    }
   ],
   "source": [
    "pred1 = model1.predict_proba(X_test.toarray())[:, 1]\n",
    "pred2 = model2.predict_proba(X_test.toarray())[:, 1]\n",
    "print roc_auc_score(y_test, pred1)\n",
    "print roc_auc_score(y_test, pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
